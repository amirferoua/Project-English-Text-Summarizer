# Project-English-Text-Summarizer

This repository is part of my blog post [Transformers â€“ Project: English text summarizer](https://mathchine-learning.blogspot.com/2020/11/transformers-project-english-text.html). Please, enter the link for a detailed description.

In this ipynb, we create a text summarizer using a transformer model. Transformers are sequence to sequence models relying entirely on attention. They work through an encoder-decoder architecture with multi-head attention layers, which draw global dependencies between input and output sequences (encoder-decoder attention) and between different positions of a single sentence (self-attention). 
